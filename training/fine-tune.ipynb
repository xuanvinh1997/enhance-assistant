{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prerequisites\n",
    "- Conda\n",
    "Run the following command to create env:\n",
    "```sh\n",
    "conda create -n assistant python~=3.10\n",
    "conda activate assitant\n",
    "conda install pytorch::pytorch torchvision torchaudio -c pytorch\n",
    "pip install autotrain-advanced\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-30 15:50:26\u001b[0m | \u001b[36mautotrain.cli.run_llm\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m344\u001b[0m - \u001b[1mRunning LLM\u001b[0m\n",
      "\u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[32m2024-05-30 15:50:26\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m180\u001b[0m - \u001b[33m\u001b[1mParameters supplied but not used: deploy, version, inference, config, train, backend, func\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-30 15:50:26\u001b[0m | \u001b[36mautotrain.backends.local\u001b[0m:\u001b[36mcreate\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mStarting local training...\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-30 15:50:26\u001b[0m | \u001b[36mautotrain.commands\u001b[0m:\u001b[36mlaunch_command\u001b[0m:\u001b[36m386\u001b[0m - \u001b[1m['accelerate', 'launch', '--num_machines', '1', '--num_processes', '1', '--mixed_precision', 'no', '-m', 'autotrain.trainers.clm', '--training_config', 'custom-phi-3-mini/training_params.json']\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-30 15:50:26\u001b[0m | \u001b[36mautotrain.commands\u001b[0m:\u001b[36mlaunch_command\u001b[0m:\u001b[36m387\u001b[0m - \u001b[1m{'model': 'microsoft/Phi-3-mini-4k-instruct', 'project_name': 'custom-phi-3-mini', 'data_path': 'mzbac/function-calling-phi-3-format-v1.1', 'train_split': 'train', 'valid_split': None, 'add_eos_token': False, 'block_size': -1, 'model_max_length': 1024, 'padding': None, 'trainer': 'sft', 'use_flash_attention_2': False, 'log': 'none', 'disable_gradient_checkpointing': False, 'logging_steps': -1, 'evaluation_strategy': 'epoch', 'save_total_limit': 1, 'auto_find_batch_size': False, 'mixed_precision': None, 'lr': 0.0002, 'epochs': 1, 'batch_size': 2, 'warmup_ratio': 0.1, 'gradient_accumulation': 1, 'optimizer': 'adamw_torch', 'scheduler': 'linear', 'weight_decay': 0.0, 'max_grad_norm': 1.0, 'seed': 42, 'chat_template': None, 'quantization': None, 'target_modules': 'all-linear', 'merge_adapter': False, 'peft': True, 'lora_r': 16, 'lora_alpha': 32, 'lora_dropout': 0.05, 'model_ref': None, 'dpo_beta': 0.1, 'max_prompt_length': 128, 'max_completion_length': None, 'prompt_text_column': 'prompt', 'text_column': 'text', 'rejected_text_column': 'rejected', 'push_to_hub': False, 'username': None, 'token': None}\u001b[0m\n",
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-30 15:50:33\u001b[0m | \u001b[36mautotrain.trainers.clm.train_clm_sft\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mStarting SFT training...\u001b[0m\n",
      "Downloading readme: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 282/282 [00:00<00:00, 1.03MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 101M/101M [00:18<00:00, 5.43MB/s]\n",
      "Generating train split: 100%|â–ˆ| 112390/112390 [00:00<00:00, 389270.99 examples/s\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-30 15:51:01\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m388\u001b[0m - \u001b[1mTrain data: Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 112390\n",
      "})\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-30 15:51:01\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m389\u001b[0m - \u001b[1mValid data: None\u001b[0m\n",
      "tokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.17k/3.17k [00:00<00:00, 21.9MB/s]\n",
      "tokenizer.model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500k/500k [00:00<00:00, 4.66MB/s]\n",
      "tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.84M/1.84M [00:01<00:00, 1.24MB/s]\n",
      "added_tokens.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 293/293 [00:00<00:00, 961kB/s]\n",
      "special_tokens_map.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 568/568 [00:00<00:00, 2.64MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-30 15:51:05\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mconfigure_logging_steps\u001b[0m:\u001b[36m461\u001b[0m - \u001b[1mconfiguring logging steps\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-30 15:51:05\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mconfigure_logging_steps\u001b[0m:\u001b[36m474\u001b[0m - \u001b[1mLogging steps: 25\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-30 15:51:05\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mconfigure_training_args\u001b[0m:\u001b[36m479\u001b[0m - \u001b[1mconfiguring training args\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-30 15:51:05\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mconfigure_block_size\u001b[0m:\u001b[36m542\u001b[0m - \u001b[1mUsing block size 1024\u001b[0m\n",
      "/Users/vinhpx/miniconda3/envs/assistant/lib/python3.12/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-30 15:51:05\u001b[0m | \u001b[36mautotrain.trainers.clm.train_clm_sft\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mloading model config...\u001b[0m\n",
      "/Users/vinhpx/miniconda3/envs/assistant/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 904/904 [00:00<00:00, 11.2MB/s]\n",
      "configuration_phi3.py: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10.4k/10.4k [00:00<00:00, 7.14MB/s]\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n",
      "- configuration_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-30 15:51:07\u001b[0m | \u001b[36mautotrain.trainers.clm.train_clm_sft\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1mloading model...\u001b[0m\n",
      "modeling_phi3.py: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 73.8k/73.8k [00:00<00:00, 167kB/s]\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n",
      "- modeling_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "model.safetensors.index.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16.3k/16.3k [00:00<00:00, 6.21MB/s]\n",
      "Downloading shards:   0%|                                 | 0/2 [00:00<?, ?it/s]\n",
      "model-00001-of-00002.safetensors:   0%|             | 0.00/4.97G [00:00<?, ?B/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   0%|    | 10.5M/4.97G [00:02<17:55, 4.62MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   0%|    | 21.0M/4.97G [00:04<18:45, 4.40MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   1%|    | 31.5M/4.97G [00:07<19:07, 4.31MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   1%|    | 41.9M/4.97G [00:09<19:01, 4.32MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   1%|    | 52.4M/4.97G [00:11<17:58, 4.56MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   1%|    | 62.9M/4.97G [00:13<17:20, 4.72MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   1%|    | 73.4M/4.97G [00:15<16:12, 5.04MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   2%|    | 83.9M/4.97G [00:17<15:54, 5.12MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   2%|    | 94.4M/4.97G [00:19<15:02, 5.41MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   2%|     | 105M/4.97G [00:20<14:24, 5.63MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   2%|     | 115M/4.97G [00:22<13:36, 5.95MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   3%|â–    | 126M/4.97G [00:24<13:22, 6.04MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   3%|â–    | 136M/4.97G [00:26<14:08, 5.70MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   3%|â–    | 147M/4.97G [00:28<15:19, 5.25MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   3%|â–    | 157M/4.97G [00:30<15:09, 5.29MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   3%|â–    | 168M/4.97G [00:32<14:07, 5.67MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   4%|â–    | 178M/4.97G [00:33<13:49, 5.78MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   4%|â–    | 189M/4.97G [00:36<14:40, 5.43MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   4%|â–    | 199M/4.97G [00:38<14:54, 5.34MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   4%|â–    | 210M/4.97G [00:40<15:46, 5.03MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   4%|â–    | 220M/4.97G [00:42<16:17, 4.86MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   5%|â–    | 231M/4.97G [00:44<15:41, 5.04MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   5%|â–    | 241M/4.97G [00:46<14:04, 5.60MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   5%|â–Ž    | 252M/4.97G [00:47<13:50, 5.69MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   5%|â–Ž    | 262M/4.97G [00:49<13:43, 5.72MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   5%|â–Ž    | 273M/4.97G [00:51<14:41, 5.33MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   6%|â–Ž    | 283M/4.97G [00:53<14:38, 5.34MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   6%|â–Ž    | 294M/4.97G [00:55<14:41, 5.31MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   6%|â–Ž    | 304M/4.97G [00:57<14:46, 5.26MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   6%|â–Ž    | 315M/4.97G [00:59<14:35, 5.32MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   7%|â–Ž    | 325M/4.97G [01:02<15:20, 5.05MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   7%|â–Ž    | 336M/4.97G [01:04<15:11, 5.09MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   7%|â–Ž    | 346M/4.97G [01:06<14:59, 5.14MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   7%|â–Ž    | 357M/4.97G [01:08<15:20, 5.01MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   7%|â–Ž    | 367M/4.97G [01:11<16:31, 4.65MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   8%|â–    | 377M/4.97G [01:13<17:54, 4.28MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   8%|â–    | 388M/4.97G [01:16<17:58, 4.25MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   8%|â–    | 398M/4.97G [01:18<17:35, 4.34MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   8%|â–    | 409M/4.97G [01:20<17:01, 4.47MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   8%|â–    | 419M/4.97G [01:23<17:04, 4.44MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   9%|â–    | 430M/4.97G [01:25<16:45, 4.52MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   9%|â–    | 440M/4.97G [01:27<16:13, 4.66MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   9%|â–    | 451M/4.97G [01:29<15:44, 4.79MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   9%|â–    | 461M/4.97G [01:32<16:53, 4.45MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   9%|â–    | 472M/4.97G [01:35<17:44, 4.23MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  10%|â–    | 482M/4.97G [01:37<17:01, 4.40MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  10%|â–    | 493M/4.97G [01:39<16:42, 4.47MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  10%|â–Œ    | 503M/4.97G [01:42<17:25, 4.27MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  10%|â–Œ    | 514M/4.97G [01:45<18:43, 3.97MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  11%|â–Œ    | 524M/4.97G [01:47<17:13, 4.31MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  11%|â–Œ    | 535M/4.97G [01:49<16:40, 4.43MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  11%|â–Œ    | 545M/4.97G [01:51<16:26, 4.49MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  11%|â–Œ    | 556M/4.97G [01:55<20:01, 3.68MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  11%|â–Œ    | 566M/4.97G [01:58<20:07, 3.65MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  12%|â–Œ    | 577M/4.97G [02:00<18:27, 3.97MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  12%|â–Œ    | 587M/4.97G [02:02<16:36, 4.40MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  12%|â–Œ    | 598M/4.97G [02:03<14:07, 5.16MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  12%|â–Œ    | 608M/4.97G [02:05<12:43, 5.72MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  12%|â–Œ    | 619M/4.97G [02:06<11:27, 6.33MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  13%|â–‹    | 629M/4.97G [02:07<10:30, 6.89MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  13%|â–‹    | 640M/4.97G [02:08<09:51, 7.32MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  13%|â–‹    | 650M/4.97G [02:10<09:23, 7.67MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  13%|â–‹    | 661M/4.97G [02:11<08:45, 8.21MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  13%|â–‹    | 671M/4.97G [02:12<08:21, 8.57MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  14%|â–‹    | 682M/4.97G [02:14<09:27, 7.57MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  14%|â–‹    | 692M/4.97G [02:15<09:19, 7.65MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  14%|â–‹    | 703M/4.97G [02:16<09:04, 7.85MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  14%|â–‹    | 713M/4.97G [02:18<09:25, 7.54MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  15%|â–‹    | 724M/4.97G [02:19<09:22, 7.56MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  15%|â–‹    | 734M/4.97G [02:20<09:20, 7.57MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  15%|â–‹    | 744M/4.97G [02:22<08:53, 7.92MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  15%|â–Š    | 755M/4.97G [02:23<09:36, 7.32MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  15%|â–Š    | 765M/4.97G [02:25<09:20, 7.51MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  16%|â–Š    | 776M/4.97G [02:26<08:43, 8.01MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  16%|â–Š    | 786M/4.97G [02:27<08:36, 8.10MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  16%|â–Š    | 797M/4.97G [02:28<08:25, 8.25MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  16%|â–Š    | 807M/4.97G [02:29<08:09, 8.52MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  16%|â–Š    | 818M/4.97G [02:30<07:57, 8.70MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  17%|â–Š    | 828M/4.97G [02:32<07:59, 8.64MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  17%|â–Š    | 839M/4.97G [02:33<08:43, 7.89MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  17%|â–Š    | 849M/4.97G [02:35<08:34, 8.01MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  17%|â–Š    | 860M/4.97G [02:36<08:51, 7.74MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  18%|â–‰    | 870M/4.97G [02:38<10:54, 6.27MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  18%|â–‰    | 881M/4.97G [02:40<11:24, 5.98MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  18%|â–‰    | 891M/4.97G [02:42<11:56, 5.70MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  18%|â–‰    | 902M/4.97G [02:44<11:53, 5.71MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  18%|â–‰    | 912M/4.97G [02:46<10:56, 6.18MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  19%|â–‰    | 923M/4.97G [02:47<10:51, 6.22MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  19%|â–‰    | 933M/4.97G [02:49<11:09, 6.03MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  19%|â–‰    | 944M/4.97G [02:51<11:57, 5.62MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  19%|â–‰    | 954M/4.97G [02:53<12:17, 5.45MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  19%|â–‰    | 965M/4.97G [02:55<12:11, 5.48MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  20%|â–‰    | 975M/4.97G [02:57<11:27, 5.82MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  20%|â–‰    | 986M/4.97G [02:59<11:45, 5.65MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  20%|â–ˆ    | 996M/4.97G [03:01<12:15, 5.41MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  20%|â–Š   | 1.01G/4.97G [03:03<11:39, 5.67MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  20%|â–Š   | 1.02G/4.97G [03:04<11:42, 5.63MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  21%|â–Š   | 1.03G/4.97G [03:06<10:48, 6.09MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  21%|â–Š   | 1.04G/4.97G [03:07<10:34, 6.20MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  21%|â–Š   | 1.05G/4.97G [03:09<09:48, 6.67MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  21%|â–Š   | 1.06G/4.97G [03:10<09:15, 7.05MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  22%|â–Š   | 1.07G/4.97G [03:11<09:00, 7.22MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  22%|â–Š   | 1.08G/4.97G [03:13<09:05, 7.14MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  22%|â–‰   | 1.09G/4.97G [03:15<09:55, 6.52MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  22%|â–‰   | 1.10G/4.97G [03:17<10:09, 6.35MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  22%|â–‰   | 1.11G/4.97G [03:18<09:58, 6.45MB/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "!autotrain llm \\\n",
    "--train \\\n",
    "--model microsoft/Phi-3-mini-4k-instruct \\\n",
    "--data-path mzbac/function-calling-phi-3-format-v1.1 \\\n",
    "--lr 2e-4 \\\n",
    "--batch-size 2 \\\n",
    "--epochs 1 \\\n",
    "--trainer sft \\\n",
    "--project-name custom-phi-3-mini \\\n",
    "--peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
